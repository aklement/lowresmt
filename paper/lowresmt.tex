%
% File acl-hlt2011.tex
%
% Contact: gdzhou@suda.edu.cn
%%
%% Based on the style files for ACL2008 by Joakim Nivre and Noah Smith
%% and that of ACL2010 by Jing-Shin Chang and Philipp Koehn


\documentclass[11pt]{article}
\usepackage{acl-hlt2011}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{url}
\usepackage{color}
\usepackage[vlined,figure]{algorithm2e}
\usepackage{graphicx} 

\DeclareMathOperator*{\argmax}{arg\,max}

\definecolor{red}{rgb}{1,0,0}

\newcommand{\mnote}[1]{\marginpar{%
  \vskip-\baselineskip
  \raggedright\footnotesize
  \itshape\hrule\smallskip\tiny{#1}\par\smallskip\hrule}}  

\newcommand{\mtodo}[1]{\mnote{\textcolor{red}{#1}}}
\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}}
\newcommand{\secref}[1]{Section~\ref{#1}}
\newcommand{\tabref}[1]{Table~\ref{#1}}
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\code}[1]{{\small \tt #1}}
\newcommand{\emq}[1]{\emph{``#1''}}
\newcommand{\bm}{\boldsymbol}
\def\bs#1{\boldsymbol{#1}}

\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{Toward Statistical Machine Translation without Parallel Corpora}

\author{}
%\author{Person\\
%  University of Awesome\\
%  Location\\
%  {\tt person@email}  \And
%  Person\\
%  University of Awesome\\
%  Location\\
%  {\tt  person@email}   \And
%  Person\\
%  University of Awesome\\
%  Location\\
%  {\tt  person@email}}

\date{}

\begin{document}
\maketitle
\begin{abstract}
The parameters of statistical translation models of are typically estimated from bilingual parallel corpora.   In this paper we explore the idea of estimating the parameters of a phrase-based statistical machine translation system from monolingual corpora.  Existing research on inducing bilingual dictionaries from monolingual texts has largely focused on learning the translations of individual, high frequency words.   We extend it to estimate all the parameters of phrase-based translation: phrasal translation pairs, and their translation and re-ordering probabilities.  We begin with a fixed phrase-table and perform lesion experiments that show how much translation performance decreases when model parameters are removed, and how much of that loss can be restored when monolingually-estimated equivalents are added.  We then analyze challenges of  inducing the phrase table and ...

% Large volumes of parallel text are required before translation models produce high quality translations, but sufficiently large corpora are available for  very few language pairs.  On the other hand, monolingual data is widely available and cheap to collect.  In this work, we propose a set of cues derived from monolingual resources and systematically introduce them in a phrase-based machine translation pipeline \emph{in place} of features induced from parallel data.  We show on two language pairs that monolingual data can take us a long way toward  inducing a high quality machine translation system.

%The parameters of statistical translation models of are estimated from bilingual parallel corpora.    Large volumes of parallel text are required before translation models produce high quality translations, but sufficiently large corpora are available for  very few language pairs.  On the other hand, monolingual data is widely available and cheap to collect.  In this work, we propose a set of cues derived from monolingual resources and systematically introduce them in a phrase-based machine translation pipeline \emph{in place} of features induced from parallel data.  We show on two language pairs that monolingual data can take us a long way toward  inducing a high quality machine translation system.

%Current statistical machine translation methods rely on very large parallel corpora to achieve state of the art performance.  However, such resources are only available for very few language pairs as they are very expensive to obtain.  On the other hand, monolingual data (such as newswire) is widely available and cheap to collect.  In this work, we propose a set of cues derived from monolingual resources and systematically introduce them in a phrase-based machine translation pipeline \emph{in place} of features induced from parallel data.  We show on two language pairs that monolingual data can take us a long way toward  inducing a high quality machine translation system.\mtodo{Need a more concrete statement.}
\end{abstract}

% ------------------------------------------------

\section{Introduction} \label{sect:intro}
Current statistical machine translation (SMT) methods (e.g. \cite{Koehn:2003,Chiang:2005}) crucially rely on vast amounts of sentence aligned translations in order to achieve state of the art performance.  These resources are only available for very few language pairs because producing them in sufficient quantities is an expensive and time consuming endeavor.  Moreover, the SMT system performance tends to drop if test data comes from a different domain then the parallel data used in training\mtodo{Need a good MT adaptation reference}.  One general idea to deal with data sparsity is to attempt to collect (more) parallel data automatically (e.g. \cite{Munteanu:2006,Smith:2010,Uszkoreit:2010}).\mtodo{Need a better "however" sentence.}  However, some assumptions are typically made about the comparable bilingual corpus (such as document level alignment) used for mining near parallel text fragments.

In this work, we approach the problem from an entirely different perspective: we use monolingual resources directly to induce an end-to-end statistical machine translation system.  In particular, we extend a long line of work on inducing translation lexicons (e.g. \cite{Rapp:1995,Fung:1998,Koehn:2000,Klementiev:2006b,Haghighi:2008,Mimno:2009}) to induce translation features, and propose a novel algorithm for estimating reordering features for the phrase-based machine translation framework \cite{Koehn:2003}.  Much of the prior work on lexicon induction is motivated by the resource constrained SMT, however, to the best of our knowledge, this work is the first attempt to extend and apply these ideas in an end-to-end machine translation pipeline.

%The rest of the paper is organized as follows.  \secref{sect:bckg} begins with the relevant background on the phrase-based SMT framework we will use in the rest of the paper and continues to give a brief overview of the existing work on inducing translation lexicons. \secref{sect:mono} motivates and introduces translation and reordering features induced from monolingual data alone.   \secref{sect:exp} studies the informativeness of these features as they are added to the Machine Translation pipeline.  Finally, \secref{sect:conc} concludes and discusses directions for future work.

In this paper we:
\begin{itemize}
\item Analyze the challenge of using bilingual lexicon induction for statistical machine translation (performance on low frequency items, moving from words to phrases, and $n^2$ comparisons).
\item Extend bilingual lexicon induction to phrasal translations, and scale it to extract translations for 30,000 phrases (which naively require tens of billions of phrase comparisons). 
\item Perform a set of lesion experiments where all feature functions are dropped from from a phrase table, and then replaced with monolingually estimated equivalents.
\item Report end-to-end translation quality with a fixed phrase-table with monolingually estimated parameters and for a fully monolingually induced system.
\end{itemize}

% ------------------------------------------------
\section{Related Work} \label{sect:related-work}

% ------------------------------------------------

\section{Background} \label{sect:bckg}
\subsection{Parameters of phrase-based SMT} \label{sect:bckg:smt}

Review phrase-based setup a-la \cite{Koehn:2003}.

\begin{itemize}
\item Log linear formulation:
  \begin{eqnarray*}
    p(\mathbf{e} | \mathbf{f}) & \propto & \exp \sum_{i=1}^{n}{\lambda_i h_i (\mathbf{e}, \mathbf{f})} \label{log-linear-formulation}\\
    \hat{e} & = & \argmax_{e}{p(\mathbf{e} | \mathbf{f}) }
  \end{eqnarray*}

\item \emph{Phrase extraction}.  Size of the phrase table and maximum phrase length: show our plot of performance vs. max phrase length.  Mention what we choose max phrase length 3 for our experiments.

\item \emph{Phrase features}.  Phrase translation probability and lexical translation probability.

\item \emph{Lexicalized reordering features}.  See \figref{fig:reorderfeats}.

\item \emph{Other features}. Language model, penalties.

\end{itemize}

\begin{figure}[t]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{../figures/reorderfeats/reorderfeats.pdf}}
\caption{Example alignment along with three kinds of orientations: monotone (m), swap (s), and discontinuous (d). }
\label{fig:reorderfeats} 
\end{center}
%\vskip -0.2in
\end{figure}\mtodo{\figref{fig:reorderfeats}: switch rows and columns to be consistent with \figref{fig:monoreord}?}

In this work, we will use the same general mathematical formulation, but propose alternative features derived directly from monolingual data.

 \subsection{Lexicon Induction} \label{sect:bckg:lexind}
 
Let us now briefly review relevant prior work on lexicon induction.  In \secref{sect:mono} we will build on this this work specifically to propose alternatives to some of the types of features we outlined in \secref{sect:bckg:smt}.

\todo{Alex: Fill in.}

Most of the previous work evaluates results on a small set of hand selected words (e.g. 100 nouns in \cite{Rapp:1995}).  However, if the objective is to induce large translation tables, as it is in our work, the reported results can be misleading. \todo{Describe \figref{fig:lexinduct}}. \mtodo{Diss prev work, but make sure that it comes across that these features are informative.}

\begin{figure}[t]
%\vskip 0.1in
\begin{center}
\centerline{\includegraphics[scale=0.45]{../figures/lexinduct/lexinduct.pdf}} %[width=\columnwidth]
\caption{Accuracy on wikipedia for most frequent and random 1000 source words. }
\label{fig:lexinduct}
\end{center}
\vskip -0.2in
\end{figure}
 
% ------------------------------------------------

\section{Parameter Estimation from Monolingual Data} \label{sect:mono}

\subsection{Phrase extraction}  \label{sect:extract}

Building on the many successful efforts in bilingual lexicon induction from monolingual corpora, we compile a table of {\it phrasal} translations from monolingual corpora. Current methods for inducing a bilingual lexicon, or, equivalently, a unigram phrase translation table, are computationally expensive as each source and target word pair must be scored. That is, the models must  compute $|V_s| * |V_t|$ similarity scores, where $|V_s|$ and $|V_t|$ are the sizes of the source and target vocabularies, respectively. As we search for longer phrase pairs, this search space increases exponentially with the size of the  {\it n}-grams. Exhaustively scoring all phrase pairs up to length three requires $|V_s|^3 * |V_t|^3 $ score computations. We prune the phrase pair search space using methods from Information Retrieval, like that used in \newcite{Uszkoreit:2010}. 

First, we compose a set of all {\it n}-grams up to length three in the source side monolingual corpus and in the target side monolingual corpus. We store each source side {\it n}-gram's frequency along with the phrase. We also store each target side {\it n}-gram in a frequency inverted index. We index target side phrases by frequencies within a band of their actual observed frequency in the monolingual corpus. Additionally, for the target side {\it n}-grams, we look up each word in the phrase in our bilingual dictionary, and we store a set of source side words that translate into any word in the phrase. \mtodo{discuss dictionary(ies)}

These sets of observed phrases, frequencies, and the bilingual dictionary together allow us to effectively filter our search space using combinations of inverted indices. Given a source side test set, we collect  {\it n}-grams up to length three (in our Urdu test set, there are 36,423 phrases), as we did for the monolingual corpora. We then look up each source side phrase's monolingual corpus frequency and consult the target side inverted index to find a list of target side phrases that occur in the same frequency band. Of those target side phrases, we keep only the ones with at least one \mtodo{discuss dictionary parameters} word translating into a word in the source side phrase. 

This method of pruning the phrase pair search space involves two manually tuned parameters. In order to evaluate the filtered phrase tables and tune the parameters, we used the Moses decoder's trace function to find the set of phrases used in decoding an Urdu test set. \mtodo{data details?} We compare our filtered phrase tables to this set of phrase translation rules and attempt to maintain as many of them as possible, while pruning the set of phrase pairs down to manageable size. 

Our baseline phrase table is generated using a bilingual dictionary. For each Urdu test set phrase up to length three, we generated English phrases from all combinations of dictionary translations and all possible reorderings. For the baseline and our pruning methods, the number of filtered phrase pairs and the percent of phrases used by the Moses decoder not pruned away are given in Table \ref{table:prune}.  

\begin{table*}
\small
\begin{center}
\label{table:prune}
\begin{tabular}{|c|c|c|c|c|}
\hline
Pruning filters & Phrase Pairs & Percent of total search space & Findable types & Findable tokens \\
\hline
Unpruned phrase table & 37,322,465,985 & 100\% & 100\% & 100\% \\
Baseline phrase pairs & 29245036 & 0.08\% & 15.37 & 25.07 \\
Frequency-based pruning & 4,450,429,494 & 11.92\% & 85.21\% & 87.48\% \\
Frequency and Dictionary pruning & 1,436,823,109 & 3.85\% & 57.79\% & 58.76\% \\
\hline
\end{tabular}
\caption{This shows the tradeoff between pruning the phrase pair search space and the accuracy of the final set of phrase pairs. The findable types and tokens measures refer to the percent of phrase types and tokens used by Moses to decode a test set that are not pruned away.}
\end{center}
\end{table*}


Second round of pruning: after monolingual feature extraction, before re-ordering estimation. Needs to be discussed after explanation of those methods? 

\subsection{Phrase scoring} \label{sect:score}

In place of phrase translation probabilities estimated from bilingual alignments, we propose to compute similarity scores computed (almost) solely from monolingual resources.\\

\begin{figure}
\centerline{\mbox{\includegraphics[width= \columnwidth]{../figures/contextual/contextual}}}
\caption{Lexicon induction using contextual information. First, contextual vectors are projected using a small seed dictionary and then compared with the target language candidates.}
\label{fig:contextual}
\end{figure}

\noindent\emph{Contextual similarity}.  We extend the vector space approach of \cite{Rapp:1999} to compute similarity between \emph{phrases} in source and target language.  More formally, assume that $(f_{1}, f_{2}, \dots f_{N})$ and $(e_{1}, e_{2}, \dots e_{M})$ are (arbitrarily indexed) source and target vocabularies, respectively.  A source phrase $f$ (target phrase $e$) is represented with an $N$ ($M$) dimensional vector.  Only the components corresponding to words that appear in the context of $f$ ($e$) in data take on non-zero values, which typically measure how ``unique'' a word is to the context in the dataset.  Next, $f$'s contextual vector is projected by mapping each component to a component in the target space corresponding to its translation (taken from a small seed dictionary), but retaining the source component value.  Finally, the pair ($f, e$) is scored by computing similarity between the (projected) source and target vectors.  Various means of computing the component values and vector similarity measures have been proposed in literature (e.g. \cite{Rapp:1999,Fung:1998}).  While the quality of the resulting induced lexicon depends on the data, we found the following to work best in our experiments.  We compute the value of the $k$-th component of $f$'s contextual vector  as follows: 

\begin{equation*}
w_{k}^{(i)} = n_{i,k} \times (log( {n / n_{i}}) + 1)
\end{equation*}

\noindent where $n_{i,k}$ and $f_{k}$ are the number of times $f_{k}$ appears in the context of $f_{i}$ and in the entire corpus, and $n$ is the maximum number of occurrences of any word in the data.  Intuitively, the more frequently $f_{k}$ appears with $f_{i}$ and the less common it is in the corpus in general, the higher its component value.  Similarity between two resulting vectors is measured as a cosine of the angle between them.\\

\begin{figure}[h]
\centerline{\mbox{\includegraphics[width= \columnwidth]{../figures/temporal/temporal}}}
\caption{Temporal histograms of the English word {\em terrorist}, its Spanish translation {\em terrorista}, and the words {\em ataques}  (attacks) and {\em riqueza} (wealth) collected from a subset of the Europarl corpus. While the correct translation has a better temporal match, the word {\em ataques} is often used around the same dates and shares a number of peaks of occurrences with the word {\em terrorist}.  The third word has a distinctly different temporal signature.}
\label{fig:temporal}
\end{figure}

\noindent\emph{Temporal similarity}. Online content is often published along with temporal information: news feeds, for example, are comprised of news stories annotated with date and time of publication.  The feeds are specialized for the target geographical locations and vary in content across languages.  Still, many events are deemed relevant to multiple audiences and the news stories related to them appear in several languages, although rarely as direct translations of one another.  Phrases associated with these events will appear with increased frequency in multiple languages around the dates when these events are reported.  Such weak synchronicity provides a cue about the relatedness of phrases across the two languages.  In order to score a pair of phrases across languages, we can compute the similarity of their temporal signatures. To generate a time sequence for a given word, we first sort the set of (time-stamped) documents of our corpus into a sequence of equally sized temporal bins.  We then count the number of occurrences of a phrase in each bin.  Changing the size of the bin or computing counts in a sliding window instead can recover some accuracy if the temporal alignment between two languages in our dataset is poor \cite{Klementiev:2006b}.  Finally, we normalize the sequence and use either the cosine measure to score similarity. \\

\noindent\emph{Orthographic / phonetic similarity.} Etymologically related words often retain similar spelling across languages with the same writing system, and the edit distance can be used to measure their orthographic similarity.  We extend this idea to phrases by using word alignments within a phrase pair (see \secref{sect:extract}): we score pairs of aligned words and normalize by their average length. \mtodo{Make sure it is correct.}

We can further extend this idea to language pairs not sharing the same writing system, since many cognates and transliterated words are phonetically similar.  Following \cite{Virga:2003,Irvine:2010a}, we treat transliteration as a monotone character translation task and use a generative model\mtodo{Argue that enough training data is easy to get} to propose a transliteration of tokens in a source phrase.  Once the source words are mapped to the target writing system, the phrase similarity is computed as before.\\

Depending on the available monolingual data (and its associated metadata), various other similarity scores can be computed and added to the list (see, e.g. \cite{Schafer:2002}).

\subsection{Reordering} \label{sect:order}

In the phrase-based SMT pipeline we reviewed in \secref{sect:bckg:smt}, phrase pair orientation statistics were collected from induced word alignments.  We keep a similar lexicalized reordering model formulation, but infer its parameters from monolingual data instead.  The orientation information for a phrase pair is collected from source and target sentences containing the two phrases as well as other hypothesized translation pairs.  Given a phrase pair ($f$, $e$), the idea is to estimate the probability that other phrases preceding $f$ will precede, follow, or become discontinuous with $e$ in target sentences when translated.  Contextual phrases used to estimate these orientation features for ($f$, $e$) will be all entries in the phrase table.  Consider a simple example on \figref{fig:monoreord}: the phrase pair is ($f =$ \emq{leckte}, $e =$ \emq{leaked}), and a given pair of unaligned sentences also contains a phrase table entry ($f_{b} =$ \emq{Das undichte Rohr}, \emq{from the broken plumbing}).  In this example, the phrase $f_{b}$ preceding $f$ in the source sentence swaps order with $e$ in the target.  When collected a over large unaligned bilingual corpora, we expect the swap, monotone, and discontinuous counts to provide good estimates for the orientation features.  Note that multiple phrases may immediately precede $f$ and appear in the phrase table; however, we only use the longest of them to collect reordering counts.\mtodo{Explain why?}

\begin{figure}[t]
%\vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{../figures/monoreord/monoreord.pdf}}
\caption{Collecting phrase ordering statistics for a de-en phrase pair (\emq{leckte}, \emq{leaked}).  The longest preceding phrase \emq{Das undichte Rohr} in source, has a phrase table translation \emq{from the broken plumbing} appearing after the target phrase.}
\label{fig:monoreord}
\end{center}
\vskip -0.2in
\end{figure}

The algorithm on \figref{fig:algoreorder} estimates monotone, swap, and discontinuous orientation features $(p_m, p_s, p_d)$ for a phrase pair ($f, e$).  It begins by calling {\tt \small CollectOccurs()} to collect the longest phrase table phrases ($B_f$) preceding $f$ in source monolingual data, as well as preceding ($B_e$), following ($A_e$), and discontinuous ($D_e$) phrases with $e$ in the target language data.  For each uniques phrase $f_{b}$ preceding $f$, translations are then looked up in the phrase table $\emph{T}$.  Next, we count\footnote{$\#_{L}(x)$ returns the count of object x in list L.} how many translations $e^*$ of $f_b$ appeared before, after or were discontinuous with $e$ in the target language data.  Finally, the counts are normalized and returned. \mtodo{Be more specific about the out-of-order counts?}  The algorithm requires a single pass through the data to collect contextual phrases for the entire phrase table and its running time is quadratic in the size table.\mtodo{Check}

\SetAlFnt{\relsize{-1.5}}

\begin{algorithm}[t]

 \SetKwFunction{CollectOccurs}{CollectOccurs}
 \SetKwBlock{Body}{}{}
 \SetCommentSty{text}
 \SetFuncSty{text}

 \hrule \vskip 0.2cm

  \KwIn{Source and target phrases $f$ and $e$,\\
  \hskip 0.85cm Source and target monolingual corpora $\emph{C}_f$ and $\emph{C}_e$,\\
  \hskip 0.85cm Phrase table pairs $\emph{T} = \{(f^{(i)}, e^{(i)})\}_{i=1}^{N}$.}
  \KwOut{Orientation features ($p_m, p_s, p_d$).}
  
  \vskip 0.2cm \hrule \vskip 0.2cm

  $S_f \leftarrow$ sentences containing $f$ in $\emph{C}_f$\;
  $S_e \leftarrow$ sentences containing $e$ in $\emph{C}_e$\;
  
  $(B_f, -, -) \leftarrow \CollectOccurs(f, \cup_{i=1}^{N} f^{(i)}, S_f)$\;
  $(B_e, A_e, D_e) \leftarrow \CollectOccurs(e, \cup_{i=1}^{N} e^{(i)}, S_e)$\;
    
  $c_m = c_s = c_d = 0$\;
  
  \vskip 0.1cm 

  \ForEach{uniques $f_b$ in $B_f$} {
    \ForEach{translation $e^{*}$ of $f_b$ in $\emph{T}$} {

      $c_m = c_m + \#_{B_e}(e^{*})$\; 
       $c_s = c_s + \#_{A_e}(e^{*})$\; 
       $c_d = c_d + \#_{D_e}(e^{*})$\; 
    }
  }
  
  $c \leftarrow c_m + c_s + c_d$;
    
  \Return{$({c_m \over c}, {c_s \over c}, {c_d \over c})$}

  \vskip 0.2cm \hrule \vskip 0.2cm

  \CollectOccurs{$r$, $R$, $S$} \Body{
   $B \leftarrow ()$; $A \leftarrow ()$; $D \leftarrow ()$\;

    \ForEach{sentence $s \in S$} {
      \ForEach{occurrence of phrase $r$ in $s$} {
        $B \leftarrow B$ + $($longest preceding and in $R)$\;
        $A \leftarrow A$ + $($longest following and in $R)$\;
        $D \leftarrow D$ + $($longest discontinuous and in $R)$\;
      }
    }
    
    \Return{($B$, $A$, $D$)}\;
  }
  
  \vskip 0.2cm \hrule \vskip 0.2cm

  \caption{Estimating reordering probabilities from monolingual data.} \label{fig:algoreorder}
  %\vskip -0.2in
\end{algorithm}

% ------------------------------------------------

\section{Experiments} \label{sect:exp}

\subsection{Data}
Describe data we use in the experiments:  Europarl \cite{Koehn:2005}, Gigaword, our own crawls\footnote{Promise to distribute after publication.}.

\subsection{Single language}

\begin{enumerate}
\item {\em Phrase features}.  (a) Augment phrase scores with mono features.  If we see better performance, reduce the amount of parallel data until it matches the performance of the original system.  Make the tradeoff argument.  (b) ({\bf lesion experiments}) See how well we do with mono features alone.
\item {\em Orientation features}. Use mono orientation features.
\item {\em Induce phrase table}.
\item {\em Put everything together}.  Run the entire pipeline.
\end{enumerate}

\subsection{Big experiment}

Now, run the entire pipeline on a handful of languages extracting monolingual features from the Gigaword and our crawls.

% ------------------------------------------------

\section{Discussion} \label{sect:disc}

% ------------------------------------------------

\section{Conclusions and Future Work} \label{sect:conc}

First to make use of plentiful monolingual data to reduce the dependence on expensive parallel data.  In particular:

\begin{itemize}
\item We plan to run this on real data for low resource languages.  We are have been actively collecting newswire data, which we plan to make available to the community.
\item Showed that augmenting standard pipeline with monolingual features helps.
\item Demonstrated that monolingual features are informative enough on their own for a competitive system.
\item Proposed an algorithm for estimating orientation probabilities from monolingual data alone.
\item Build complete systems for X low-resource languages.
\end{itemize}

%\section*{Acknowledgments}
%The authors would like each other and their parents.

\bibliographystyle{acl}
% you bib file should really go here 
\bibliography{lowresmt}

\end{document}
